# Pose Guided Person Image Generation

- code1:[chuanqichen/deepcoaching: Sports Coaching from Pose Estimation (github.com)](https://github.com/chuanqichen/deepcoaching)

- code2:[sgoldyaev/DeepFashion.ADGAN (github.com)](https://github.com/sgoldyaev/DeepFashion.ADGAN)

        本文提出了一种名为PG2的人体图像生成网络，其能够将输入图像中人体姿态转换成任意指定的目标姿态。

## 创新点

1. 提出在根据参考图像和目标姿势的条件下的图像生成任务，核心目的是将图像中的人体操作成任意姿势。（本文应该算是姿态伪造这边一篇开山之作了）
2. 本文探索了多种方式类结合原始图像和姿态信息，并提出了一种掩码损失mask loss来鼓励模型专注于对人体外观的生成而不是背景部分。（这里的结合方式和mask loss在后续也启发了一些工作）
3. 作者使用了一个两阶段的生成方式来将渐进式的生成目标图像，第一个阶段专注于人体结构（姿态）的迁移（生成），第二阶段则在第一阶段的基础上使用对抗的训练方式来补充人体的细节，更加强调外观上的生成。（后续几年的工作都借鉴了这样的思想，将问题进行分解，只不过角度和实现方式上有区别，但是核心想法就是外观和姿态生成尽量独立开做）。

## 网络结构

![](D:\PersonSpace\blog\paper\img\pose%20guide%20person%20generate\模型结构.png)

### 第一阶段:姿态整合

        网络的第一阶段主要负责将输入图$I_A$ 和给定的姿态$P_B$结合起来产生一个粗糙的图像$I_{B}^{'}$,其捕获目标图像$I_B$中的人体的全局结构（姿态）,换句话说就是图像$I_{B}^{'}$的姿态和图像$I_B$一致。

#### 姿态的嵌入pose embedding

        姿态的嵌入实际上可以理解成对姿态的编码或者对姿态进行适当的表示，根据其表示的不同，其和图像$I_A$的结合也存在差别。

1. 作者使用但是最先进的姿态估测方法来获取近似的人体姿态，这样就不需要进行昂贵的姿态标注，该方法使用18个关键点来刻画人体的姿态。

2. 考虑到如果直接使用这个18个关键点信息作为输入，则模型还需要去学习将关键点映射到人体对位置上。因此，这里以这18个关键点分别生成对应的18张热力图（热力图的大小与原图像一致），每张热力图上对应的关键点位置及其半径4个像素以内区域的像素值为1，其余位置像素值为0。这样生成的18张热力图称为$P_B$

3. 这里直接将$P_B$和$I_A$连接起来作为输入，这样可以直接使用卷积层来实现外观和姿势这两种信息的整合。

#### 生成器G1

        生成器1的整体架构是一个类似U-NET的网络结构（U-NET是一种带有跳跃连接的基于卷积层的自编码器结构）。

1. 首先编码部分使用连续的堆叠的卷积层从局部邻域到更大的范围来不断整合$P_B$和$I_A$信息（这样只能实现外观信息在局部位置上的传递），因此作者又使用一个全连接层来实现相距较远的身体部分也能交换信息。（这样如果姿势的变化幅度较大也能够实现比较好的变化。）

2. 解码器部分使用了和编码器部分对称的卷积层，来生成图像$I^{'}_{B}$,编码器和解码器之间的跳跃连接能够将图片信息之间从输入传递给输出。

3. G1中的卷积层主要由简化的残差块renet_block组成，作者认为这样有助于提高生成图片的效果。

#### Pose mask loss

        在阶段一里，作者使用L1LOSS来比较生成的$I^{'}_{B}$和$I_{B}$之间差异，即以L1_LOSS作为损失函数，但是在生成图像过程中由于只使用源图像$I_A$和目标姿势$P_{B}$，因此对生气器G1而言，很难生成与目标图像$I_{B}$一致的背景（生成的图片背景会目标图像背景存在很大差异），这样的情况下直接使用L1_LOSS来作为损失函数，会很大程度上受到背景差异的影响，然后在第一阶段中背景的生成并不是我们关注的重点，我们关注的重点是人体全局结构与目标图像是否一致（即姿态是否是目标姿态）。因此作者对L1_LOSS增加了一个mask机制来增加全局结构差异在损失中的比重，进而使得模型更加专注于对全局姿态的生成。

        pose mask的生成：如下如所示，首先根据姿态估测时获得18个关键点，然后使用这些点结合图像的形态学操作来生成人体图像的掩膜$M_B$，人体部分的值为1，背景部分的值为0,在后续loss计算过程中，掩膜部分的差异会被放大。

<img title="" src="file:///D:/PersonSpace/blog/paper/img/pose guide person generate/pose_mask.png" alt="" data-align="center">

        pose mask loss：$G1(I_A,P_B)$表示生成图像$I^{'}_B$ ，将其与目标图像相减得到矩阵就是二者的差，再和$(1+M_B)$逐元素相乘，这样人体部分的差异会被放大，使得生成器G1更加专注于对人体全局结构信息的生成。

<img src="file:///D:/PersonSpace/blog/paper/img/pose%20guide%20person%20generate/mask%20loss.png" title="" alt="" data-align="center">

    虽然作者对其进行了改进，但是由于本质上还是L1_LOSS,因此还是遇到图像模糊的问题，因此L1_loss会鼓励结果趋向于所有可能情况的一个平均，因此为了生成细节上更加清晰的结果，作者引入了第二阶段来生成一个细节更加丰富的结果。

### 第二阶段：图像细节的生成

        第二阶段主要使用基于条件的对抗式图像生成网络（condition GAN）来对第一阶段上生成的图像进行细节上的补充。

#### 生成器G2

        生成器G1使用了一个全卷积层构成的U-NET网络，使用第一阶段生成结果$I_B$和原图像$I_A$作为输入。特别值得注意的是：***作者并没有直接使用生成器G2来生成一个接近于目标图像的图像，而是生成目标图像$I_B$和第一阶段生成图像$I^{'}_B$之间的差异图。*** 这样做能够使得模型更加容易训练，因为模型只需要专注于学习二者之间差异即可。

        在生成器G2部分使用U-NET网络结构中并未使用到G1中使用到全连接层，因为这里是在全局结构已经生成情况下补充细节，因此并不太需要全连接层来实现远距离信息的交换，同时全连接层的存在也会增大网络的训练难度。

#### 鉴别器

        在第二阶段中图像细节的生成使用对抗的方式，因此需要一个鉴别器来对生成图像进行分类，但是与一般将目标图像$I_B$和$I^{'}_B$作为输入进行真伪分类不同的是，***这里使用(I_A,I^{'}_{B2})和(I_A,I_B) 来作为输入***，目的是鼓励网络去学习$I^{'}_{B2}$和$I_B$之间的区别来区分出图片的真伪，而不是单纯地去学习自然图片和生成图片之间的的不同。（如果只是学习真实图片和生成图片的差异的，一个简单的会存在的问题的就是如果只使用$I^{'}_{B2}$和$I_B$作为输入来判别真假，则生成G2可以直接输出原始图片$I_A$,就可以骗过鉴别器。），这一点也在后续的方法中被使用。

#### 对抗损失

![](D:\PersonSpace\blog\paper\img\pose%20guide%20person%20generate\对抗损失.png)

        在对抗损失部分，其与一般对抗性损失的基本一致，使用二分类来作为的损失来作为对抗性的损失。

         但是前面也提到了阶段二是一个condition GAN，因此在生成器部分损失除了对抗性损失而言，还应当包括一个条件损失（即与目标图像的差异（距离））来约束生成器的生成过程。这里我们的条件损失为前面提到 pose mask loss，因此最终对生成器G2，其所使用的损失函数如下：

<img src="file:///D:/PersonSpace/blog/paper/img/pose%20guide%20person%20generate/G2的损失函数.png" title="" alt="" data-align="center">

### 训练过程

    整体的训练过程中就先训练第一阶段，在第一阶段训练完成后，使用对抗的训练的方式来训练第二阶段生成器和判别器。

## 实验

### 数据集

- DeepFashion dataset ：一个时尚穿搭的数据集，背景多为白色，图片分辨率较高。

- Market-1501:其由6个独立的摄像头拍摄的1501人的32668张图像，由于图像皆来自于监控摄像头，所以分辨率较低，场景比较复杂，难度较高。

### 相关实验

#### 定性分析

由于作者是该领域的一个开山之作，所以没有特别合适能用来进行比较的方法，因此作者主要进行一些消融实验来说明自己提出方法的有效性。

- 不同的姿势嵌入，前面我们提到作者是将姿势以18张热力图的形式嵌入到模型输入中去，然后使用一个编码器进行结合的方式。作者将这种方式与名为CE和HME方式进比较。
  
  - CE的嵌入：是将18点信息组成的向量通过两层全连接层连接到G1中全连接层生成的特征向量上。
  
  - HME：同样使用18张热力图来表示姿势信息，与本文方法不同的是，其使用一个独立编码器和一个全连接层来提取信息，在将连接到其G1中全连接层生成的原始图片的特征向量上。
  
  在比较三者不同时，作者只用G1和L1Loss条件下生成的结果，并未使用第二阶段的结果。因此这里用G1_CE_L1,G1_HME_L1,G1_L1来表示。

- pose mask loss VS L1_LOSS:作者同样在G1上比较L1_LOSS和pose mask loss对第一阶段生成的图像的影响。

- 两阶段和一阶段的生成方式：作者比较了直接使用G1+D（即以对抗方式来对G1来进行训练以直接生成目标图像）和本文提出两阶段方法G1+G2+D。

<img src="file:///D:/PersonSpace/blog/paper/img/pose%20guide%20person%20generate/定性分析.png" title="" alt="" data-align="center">

#### 定量分析

作者除了定性分析外还使用了定量分析，主要使用以下指标：

- SSIM：结构相似度，即通过比较生成图片和目标图片之间相似程度。

- IS：inception score，即使用inception v3来对生成的图片进行分类，得到图片分类的概率分布，对一张图片而言，其属于某个类别的概率越高，其IS分数越高，这保证图片能够尽可能像某个物体，而不是一个四不像，同时对大量生成图片而言，其最终属于不同类别的图像数目应尽可一致，这保证了生成图片的多样性。

- mask-SSIM：考虑背景信息的干扰，因此提出使用mask的方法来指标人体部分的结构相似度，以获得对生成图片更加准确的评估。在对deep fashion的评估中，则并未使用这一指标，因为该数据集中图像背景比较简单。

- mask-IS：同上。

<img src="file:///D:/PersonSpace/blog/paper/img/pose%20guide%20person%20generate/定量分析.png" title="" alt="" data-align="center">

#### 额外的实验

<img src="file:///D:/PersonSpace/blog/paper/img/pose%20guide%20person%20generate/额外的补充的实验结果.png" title="" alt="" data-align="center">

由于没有特别合适的方法来进行比较，作者只能选取一个多视角人体图像生成方法来作为比较方式，结果当然比较明显，作者的方法在解决该问题时具有明显优势。

作者也给出了几个失败的例子说明模型的不足：

1. 模型在面对复杂姿势变化时的效果不佳。

2. 由于数据偏科，导致生成人脸更偏向于女性人脸

3. 在人体外观比较复杂时，如第二行，源人物的外套和寸衫的颜色比较类似，导致模型在生成衣物发生了失败。

## 总结

作者的本篇论文作为该领域的开山之作吧，其将复杂问题进行分解（姿势生成和外观生成相分离），姿态的嵌入以及mask_loss的想法在后续新的工作中都有所体现，算是启发后续工作的展开。
